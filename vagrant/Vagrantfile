Vagrant.configure("2") do |config|
  config.vm.box = "bento/ubuntu-22.04" # Imagen de Ubuntu 18.04 (compatible con Ubuntu 22.04)

  config.vm.network "private_network", type: "dhcp" # Adaptador de red privada para acceso a la máquina

  config.vm.provider "virtualbox" do |vb|
    vb.memory = "2048" # Asignar 2 GB de memoria RAM a la máquina virtual (ajustar según tu sistema)
    vb.cpus = 1 # Asignar 1 núcleo de CPU a la máquina virtual (ajustar según tu sistema)
  end

  config.vm.provision "shell", inline: <<-SHELL
    # Actualizar repositorios e instalar paquetes requeridos
    sudo apt-get update
    sudo apt-get install -y openjdk-8-jdk

    # Instalar Hadoop, Hive y Spark
    # (Asegúrate de tener los enlaces de descarga actualizados para las versiones requeridas)
    wget -qO- 
https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz 
| tar xvz -C /opt
    wget -qO- 
https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz 
| tar xvz -C /opt
    wget -qO- 
https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
| tar xvz -C /opt

    # Configurar variables de entorno para Hadoop y Spark
    echo 'export HADOOP_HOME=/opt/hadoop-3.3.5' >> ~/.bashrc
    echo 'export HIVE_HOME=/opt/apache-hive-3.1.3-bin' >> 
~/.bashrc
    echo 'export SPARK_HOME=/opt/spark-3.4.1-bin-hadoop3.2' >> 
~/.bashrc
    echo 'export 
PATH=$PATH:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin' >> 
~/.bashrc

    # Cargar las variables de entorno
    source ~/.bashrc
  SHELL
end

